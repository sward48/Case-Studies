---
title: "Hamden Property Data Fairness"
author: "S&DS 425/625"
format: pdf
editor: visual
---

## Introduction

The state of Connecticut compiles property data from municipalities across the state and provides it for viewing and downloading on the state government website's CT Geodata Portal <https://geodata.ct.gov/pages/parcels>. Data can also be obtained from the property value assessor's website. For example, here is the website for Hamden, CT: <https://gis.vgsi.com/hamdenct/>.

The file [`Connecticut_CAMA_and_Parcel_Layer_3895368049124948111`](https://yaleedu-my.sharepoint.com/:u:/g/personal/brian_macdonald_yale_edu/EVAE5NykXsdLlqI4ufT6nBwBA0_oJh1jqjPioEMQ_G7Fmg?e=qoYauD) contains data obtained from CT. The property data contain information like location, mailing address, assessed property value, size of the plot of land in acres, floor space in sq ft, whether the property is commercial or residential, etc., and for homes it has details like number of bedrooms, number of bathrooms, etc., and the most recent sale price for each property, and the date of the sale. This is similar to the data you saw in S&DS 361/661, except we aren't providing shapefiles for the property boundaries.

The `sales_data.xlsx` file contains sales data for recent property sales. It is not known whether this data contains more, less, or the same sales information that is contained in the property data.

## Your goals

A Hamden resident who recently purchased his house had his assessed property value significantly increase within a year of the purchase. He began to question whether the assessed value was fair. He would like to use data to see if there is any evidence he could use to contest his assessed value.

Your goal is determine whether the assessed values for residential properties in Hamden, CT are fair and determine if there is any evidence to support contesting the assessed value. Write up your analysis and submit a short PDF report on explaining your reasoning.

Note that when an assessment company decides on a new set of assessed values for properties in a given year (typically using some sort of model), they do a sanity check and compare their assessed values to actual sale prices from the past year.

```{r}
library(janitor)
sales_df <- read.csv("sales_scraped.csv", check.names = TRUE, stringsAsFactors = FALSE)
property_df <- readRDS("hamden_property.rds")

sales_df    <- clean_names(sales_df)
property_df <- clean_names(property_df)
```

```{r}
property_keep <- c(
  "pid", "link", 
  "land_acres", "living_area", "effective_area", "total_rooms", "number_of_bedroom",
  "number_of_baths", "number_of_half_baths", "occupancy", "ayb", "eyb",
  "shape_area", "shape_length"
)
property_df <- property_df %>%
  mutate(pid = as.integer(str_extract(link, "(?<=^35650-)\\d+")))  %>%
  select(any_of(c(property_keep, "pid"))) %>% 
  filter(!is.na(pid))
# merge on pid (keep all properties; bring in sales/appraisal cols)
merged_df <- property_df %>% left_join(sales_df, by = "pid")
str(merged_df)
```

```{r}
library(dplyr)
library(lubridate)
library(ggplot2)
library(scales)

df <- merged_df %>%
  mutate(
    sale_date = ymd(sale_date),
    sale_year = year(sale_date),

    # choose the assessment snapshot closest to the sale date
    assessed_total = case_when(
      !is.na(total_2024) & sale_year >= 2024 ~ total_2024,
      !is.na(total_2023) & sale_year <= 2023 ~ total_2023,
      TRUE ~ coalesce(total_2024, total_2023)
    ),

    # core ratios
    ratio_assess = assessed_total / sale_price,    # target ~0.70 in CT
    ratio_equal  = (assessed_total / 0.70) / sale_price  # target ~1.00
  )

# keep plausible, recent sales
recent <- df %>%
  filter(
    !is.na(sale_price), sale_price > 10000, sale_price < 10000000, 
    occupancy == 1,
    sale_date >= as.Date("2023-01-01"), 
    !is.na(assessed_total)
  ) %>%
  # trim extreme outliers in ratio to stabilize metrics
  filter(between(ratio_assess, 0.2, 2))

nrow(recent)
summary(select(recent, sale_price, assessed_total, ratio_assess, ratio_equal))

# ---- IAAO metrics -----------------------------------------------------------
# COD: coefficient of dispersion (lower = more uniform).
# Use standard definition on assessment ratios (assessed / sale price)
med_r <- median(recent$ratio_assess, na.rm = TRUE)
COD   <- 100 * median(abs(recent$ratio_assess - med_r) / med_r, na.rm = TRUE)

# PRD: price-related differential. >1.03 suggests regressivity; <0.98 progressivity.
mean_r      <- mean(recent$ratio_assess, na.rm = TRUE)
wmean_ratio <- sum(recent$assessed_total, na.rm = TRUE) / sum(recent$sale_price, na.rm = TRUE)
PRD         <- mean_r / wmean_ratio

# PRB: slope of ratio ~ log(price); positive slope suggests regressivity.
fit_prb <- lm(ratio_assess ~ log(sale_price), data = recent)
PRB     <- coef(fit_prb)[2]   # slope

list(
  n_sales = nrow(recent),
  median_ratio = med_r,
  COD = COD,
  PRD = PRD,
  PRB = unname(PRB)
)

# ---- visuals ----------------------------------------------------------
# 1) Scatter: equalized value vs sale price with y = x
ggplot(recent, aes(x = sale_price, y = assessed_total / 0.70)) +
  geom_point(alpha = 0.35) +
  geom_abline(slope = 1, intercept = 0, color = "gray40") +
  scale_x_continuous(labels = dollar_format()) +
  scale_y_continuous(labels = dollar_format()) +
  labs(x = "Sale price", y = "Equalized assessed value (assessed / 0.70)",
       title = "Market test: Equalized assessment vs. sale price")

# 2) Distribution of ratios (assessment / sale price) â€” target centered near 0.70
ggplot(recent, aes(x = ratio_assess)) +
  geom_histogram(bins = 40, color = "white") +
  geom_vline(xintercept = 0.70, linetype = 2, color = "firebrick") +
  geom_vline(xintercept = med_r, linetype = 1, color = "steelblue") +
  labs(x = "Assessment ratio (assessed / sale price)",
       y = "Count",
       title = "Assessment ratios vs. target 0.70",
       subtitle = paste0("Median = ", round(med_r, 3), 
                         " | COD = ", round(COD, 1), 
                         " | PRD = ", round(PRD, 3)))

# 3) Price-related bias view
ggplot(recent, aes(x = sale_price, y = ratio_assess)) +
  geom_point(alpha = 0.25) +
  geom_smooth(method = "lm", se = TRUE, color = "black") +
  scale_x_continuous(labels = dollar_format()) +
  labs(x = "Sale price", y = "Assessment ratio",
       title = "Price-related bias: ratio vs price",
       subtitle = paste0("Slope (PRB) = ", round(PRB, 4)))

# 4) Bias view: by year built
ggplot(recent, aes(x = eyb, y = ratio_assess)) +
  geom_point(alpha = 0.25) +
  geom_smooth(method = "lm", se = TRUE, color = "black") +
  scale_x_continuous() +
  labs(x = "eyb", y = "Assessment ratio",
       title = "Building age-related bias: ratio vs eyb")


# PRB-style slope: ratio ~ log(effective_area)
PRB_eff <- unname(coef(lm(ratio_assess ~ log(effective_area), data = recent))[2])
# 4) Bias view: by size  -------------------------------------------------------
ggplot(recent, aes(x = effective_area, y = ratio_assess)) +
  geom_point(alpha = 0.25) +
  geom_smooth(method = "lm", se = TRUE, color = "black") +
  scale_x_continuous(labels = label_comma()) +
  labs(x = "effective_area", y = "Assessment ratio",
       title = "Building area bias: ratio vs effective area",
       subtitle = paste0("Slope (PRB, using log area) = ", round(PRB_eff, 4)))
```

```{r}
# Both mean and median are actually significantly lower than 0.7 of the recent sales prices
t.test(recent$ratio_assess, mu = 0.70)
wilcox.test(recent$ratio_assess - 0.70, conf.int = TRUE)
```